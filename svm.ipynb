{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_dir, paths_only=False):\n",
    "    \"\"\"\n",
    "    Either loads images as numpy arrays or the paths.\n",
    "    data_dir is expected to contain data split by class in subfolders\n",
    "    \"\"\"\n",
    "\n",
    "    # Read classes\n",
    "    classes = sorted(os.listdir(data_dir))\n",
    "\n",
    "    # Initialize train and test data list\n",
    "    train_images, test_images = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "\n",
    "    # Set training data ratio\n",
    "    train_ratio = 0.7\n",
    "\n",
    "    for idx, class_name in enumerate(tqdm(classes)):\n",
    "\n",
    "        # Read images in class folder\n",
    "        class_data = os.listdir(os.path.join(data_dir, class_name))\n",
    "\n",
    "        # Train data are the first images in class-folder\n",
    "        for img_name in class_data[:int(train_ratio*len(class_data))]:\n",
    "\n",
    "            if paths_only: # Get only images paths\n",
    "                train_images.append(os.path.join(data_dir, class_name, img_name))\n",
    "\n",
    "            else: # Load data and append it to train_images list\n",
    "                image = Image.open(os.path.join(data_dir, class_name, img_name))\n",
    "                train_images.append(np.array(image))\n",
    "\n",
    "        # Identify every image in the flattened list train_images with its class\n",
    "        train_labels += [idx]*int(train_ratio*len(class_data))\n",
    "\n",
    "        # Test data are the last images\n",
    "        for img_name in class_data[int(train_ratio*len(class_data)):]:\n",
    "\n",
    "            if paths_only: # Get only images paths\n",
    "                test_images.append(os.path.join(data_dir, class_name, img_name))\n",
    "\n",
    "            else: # Load data and append it to test_images list\n",
    "                image = Image.open(os.path.join(data_dir, class_name, img_name))\n",
    "                test_images.append(np.array(image))\n",
    "\n",
    "        # Identify every image in the flattened list test_images with its class\n",
    "        test_labels += [idx]*(len(class_data) - int(train_ratio*len(class_data)))\n",
    "\n",
    "    if paths_only: # Return it as numpy arrays\n",
    "        return (classes,\n",
    "                np.array(train_images), np.array(train_labels),\n",
    "                np.array(test_images), np.array(test_labels))\n",
    "    else: # Concatenate images into a single float numpy array of shape (#imgs, width, height, channels)\n",
    "        train_images = np.stack(train_images).astype(float)\n",
    "        train_labels = np.array(train_labels)\n",
    "        test_images = np.stack(test_images).astype(float)\n",
    "        test_labels = np.array(test_labels)\n",
    "        return classes, train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 251.11it/s]\n",
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2165it [08:55,  4.04it/s]\n",
      "931it [05:24,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Initialing compute device (use GPU if available).\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "normalization_std = [0.229, 0.224, 0.225]\n",
    "normalization_mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.RandomResizedCrop(224),\n",
    "                              transforms.Normalize(mean=normalization_mean,\n",
    "                                                   std=normalization_std)])\n",
    "\n",
    "classes, train_images, train_labels, test_images, test_labels = \\\n",
    "    load_data(\"Data\", True)\n",
    "\n",
    "# Utility function.\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    # Fake batch dimension required to fit network's input dimensions.\n",
    "    image = test_transforms(image).unsqueeze(0)\n",
    "\n",
    "    return image.to(device)\n",
    "\n",
    "model = models.vgg16(weights=VGG16_Weights).features.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "red_train_img = []\n",
    "for image_path, label in tqdm(zip(train_images, train_labels)):\n",
    "    image = image_loader(image_path)\n",
    "    features = model(image)\n",
    "    red_train_img.append(features.data.detach().cpu().numpy().flatten())\n",
    "\n",
    "red_test_img = []\n",
    "for image_path, label in tqdm(zip(test_images, test_labels)):\n",
    "    image = image_loader(image_path)\n",
    "    features = model(image)\n",
    "    red_test_img.append(features.data.detach().cpu().numpy().flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0064788353151242145 0.024406183086212077 0.02273851243901476\n"
     ]
    }
   ],
   "source": [
    "# Check vgg is working correctly\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "km = KMeans(n_clusters=len(classes))\n",
    "\n",
    "clusters = km.fit_predict(red_train_img)\n",
    "\n",
    "ari = adjusted_rand_score(train_labels, clusters)\n",
    "nmi = normalized_mutual_info_score(train_labels, clusters)\n",
    "ami = adjusted_mutual_info_score(train_labels, clusters)\n",
    "\n",
    "print(ari,nmi,ami)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b [0 0 0 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Save reduced data\n",
    "\n",
    "data2save = {\n",
    "    \"classes\": classes,\n",
    "    \"train_data\": red_train_img,\n",
    "    \"train_labels\": train_labels,\n",
    "    \"test_data\": red_test_img,\n",
    "    \"test_labels\": test_labels\n",
    "}\n",
    "import pickle\n",
    "\n",
    "with open(\"here.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data2save,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the list: (2165, 25088)\n"
     ]
    }
   ],
   "source": [
    "#Open reduced data\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"here.pickle\", \"rb\") as f:\n",
    "    b = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "classes=b[\"classes\"]\n",
    "train_images=b[\"train_data\"]\n",
    "train_labels=b[\"train_labels\"]\n",
    "test_images=b[\"test_data\"]\n",
    "test_labels=b[\"test_labels\"]\n",
    "\n",
    "\n",
    "np_array = np.array(train_images)\n",
    "print(\"Shape of the list:\", np_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\evida\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Initialize parameters.\n",
    "kernel = 'linear'\n",
    "max_iteration = 1000\n",
    "\n",
    "# Initialise an SVM classification model for each one of the 4 classes.\n",
    "models = [SVC(kernel=kernel, max_iter=max_iteration, probability=True) for i in range (4)]\n",
    "\n",
    "# Train the models.\n",
    "for i in range(4):\n",
    "  models[i].fit(train_images, train_labels==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23 194   0  54]\n",
      " [ 12 206   0  56]\n",
      " [ 11 104   0  17]\n",
      " [ 16 177   0  61]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted_scores = []\n",
    "for i in range(4):\n",
    "    predicted_scores.append(models[i].predict_proba(test_images)[:,1])\n",
    "\n",
    "predicted_scores = np.asarray(predicted_scores)\n",
    "predicted = np.argmax(predicted_scores,axis=0)\n",
    "\n",
    "conf_mat = confusion_matrix(test_labels, predicted)\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"Accuracy =\", accuracy_score(test_labels,predicted))\n",
    "print(\"Precision =\", precision_score(test_labels,predicted,average='macro'))\n",
    "print(\"Recall =\", recall_score(test_labels,predicted,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "[[199  45   5  22]\n",
    "\n",
    "[ 70 149  10  45]\n",
    "\n",
    "[ 12  24  86  10]\n",
    "\n",
    "[ 12  35   5 202]]\n",
    "\n",
    "Accuracy = 0.6831364124597207\n",
    "\n",
    "Precision = 0.7008621963326671\n",
    "\n",
    "Recall = 0.6812259264194301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Initialize parameters.\n",
    "kernel = 'rbf'\n",
    "max_iteration = 1000\n",
    "\n",
    "# Initialise an SVM classification model for each one of the 4 classes.\n",
    "models = [SVC(kernel=kernel, max_iter=max_iteration, probability=True) for i in range (4)]\n",
    "\n",
    "# Train the models.\n",
    "for i in range(4):\n",
    "  models[i].fit(train_images, train_labels==i)\n",
    "\n",
    "predicted_scores = []\n",
    "for i in range(4):\n",
    "    predicted_scores.append(models[i].predict_proba(test_images)[:,1])\n",
    "\n",
    "predicted_scores = np.asarray(predicted_scores)\n",
    "predicted = np.argmax(predicted_scores,axis=0)\n",
    "\n",
    "conf_mat = confusion_matrix(test_labels, predicted)\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"Accuracy =\", accuracy_score(test_labels,predicted))\n",
    "print(\"Precision =\", precision_score(test_labels,predicted,average='macro'))\n",
    "print(\"Recall =\", recall_score(test_labels,predicted,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "[[205  46   3  17]\n",
    "\n",
    " [ 74 146   6  48]\n",
    "\n",
    " [ 16  25  80  11]\n",
    "\n",
    " [ 11  33   2 208]]\n",
    "\n",
    "Accuracy = 0.686358754027927\n",
    "\n",
    "Precision = 0.7163624714602345\n",
    "\n",
    "Recall = 0.6785656309399986"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
